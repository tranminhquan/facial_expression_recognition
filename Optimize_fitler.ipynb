{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms import CenterCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dDF(nn.Conv2d):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride=1, padding=0, alpha=0.5, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "        super(Conv2dDF, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\n",
    "        \n",
    "        # unpair kernel_size from init super method\n",
    "#         self.kernel_size = self.kernel_size[0]\n",
    "        self.kernel_size = Parameter(torch.Tensor(self.kernel_size))\n",
    "#         self.register_parameter(name='kernel_size', param=self.kernel_size)\n",
    "        \n",
    "        self.k_plus = (math.floor((self.kernel_size[0] + 1) / 2) * 2 + 1, math.floor((self.kernel_size[1] + 1) / 2) * 2 + 1)\n",
    "        self.k_minus = (math.ceil((self.kernel_size[0] + 1) / 2) * 2 - 1, math.ceil((self.kernel_size[1] + 1) / 2) * 2 - 1)\n",
    "        \n",
    "        self.weight_plus = torch.Tensor(\n",
    "                out_channels, in_channels // groups, *self.k_plus)\n",
    "        \n",
    "        self.weight_minus = CenterCrop(self.k_minus)(self.weight_plus)\n",
    "        \n",
    "        self.delta_w = self.weight_plus - F.pad(self.weight_minus, (1,1,1,1,0,0), mode='constant', value=0)\n",
    "        self.weight = Parameter(alpha * self.delta_w + F.pad(self.weight_minus, (1,1,1,1,0,0), mode='constant', value=0))\n",
    "        \n",
    "        self.register_backward_hook(self.backward_func)\n",
    "        \n",
    "    def backward_func(self, module, grad_input, grad_output):\n",
    "        print(grad_input)\n",
    "#         print(grad_output)\n",
    "        \n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "    \n",
    "#     @staticmethod\n",
    "    def forward(self, input):\n",
    "        return self._conv_forward(input, self.weight)\n",
    "    \n",
    "# #     @staticmethod\n",
    "#     def backward(self, grad_output):\n",
    "#         \"\"\"\n",
    "#         In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "#         with respect to the output, and we need to compute the gradient of the loss\n",
    "#         with respect to the input.\n",
    "#         \"\"\"\n",
    "# #         print(ctx, grad_output)\n",
    "#         print(grad_output)\n",
    "#         return super(Conv2dDF, self).backward(grad_output)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, tensor([[[[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0074, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0073, -0.0075, -0.0073, -0.0072],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0075, -0.0076, -0.0078, -0.0076, -0.0075],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0073, -0.0072]]],\n",
      "\n",
      "\n",
      "        [[[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0074, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0073, -0.0075, -0.0073, -0.0072],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0075, -0.0076, -0.0078, -0.0076, -0.0075],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0073, -0.0072]]],\n",
      "\n",
      "\n",
      "        [[[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0074, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0073, -0.0075, -0.0073, -0.0072],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0075, -0.0076, -0.0078, -0.0076, -0.0075],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0073, -0.0072]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0074, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0073, -0.0075, -0.0073, -0.0072],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0075, -0.0076, -0.0078, -0.0076, -0.0075],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0073, -0.0072]]],\n",
      "\n",
      "\n",
      "        [[[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0074, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0073, -0.0075, -0.0073, -0.0072],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0075, -0.0076, -0.0078, -0.0076, -0.0075],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0073, -0.0072]]],\n",
      "\n",
      "\n",
      "        [[[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0074, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0074, -0.0075, -0.0074, -0.0072],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0075, -0.0077, -0.0078, -0.0077, -0.0075],\n",
      "          [-0.0074, -0.0075, -0.0077, -0.0075, -0.0074],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0074, -0.0072]],\n",
      "\n",
      "         [[-0.0072, -0.0073, -0.0075, -0.0073, -0.0072],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0075, -0.0076, -0.0078, -0.0076, -0.0075],\n",
      "          [-0.0073, -0.0075, -0.0076, -0.0075, -0.0073],\n",
      "          [-0.0072, -0.0073, -0.0075, -0.0073, -0.0072]]]]), tensor([-0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "conv = Conv2dDF(3, 64, 3, padding=2, stride=1)\n",
    "# conv = nn.Conv2d(3, 64, 3, padding=1, stride=1)\n",
    "optimizer = optim.Adam(conv.parameters(), 0.001)\n",
    "\n",
    "x = conv(torch.rand(12, 3, 48, 48))\n",
    "lasst_w = conv.weight\n",
    "\n",
    "(torch.ones((12, 64, 48, 48)) - x).mean().backward()\n",
    "optimizer.step()\n",
    "\n",
    "cur_w = conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([12, 12]) and output[0] has a shape of torch.Size([]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-374-a61a13c90311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and output[\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"] has a shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                    + str(out.shape) + \".\")\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 raise RuntimeError(\"For complex Tensors, both grad_output and output\"\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([12, 12]) and output[0] has a shape of torch.Size([])."
     ]
    }
   ],
   "source": [
    "t = linear(torch.rand(12, 23), torch.rand(12, 23))\n",
    "(torch.ones(12,12) - t).mean().backward(torch.rand(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
