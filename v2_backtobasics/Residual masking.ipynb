{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train_data_transform = transforms.Compose([\n",
    "        \n",
    "#         transforms.Lambda(lbp_transform),\n",
    "#         transforms.Lambda(combine_lbp_hog),\n",
    "#         transforms.ToPILImage(),\n",
    "        # transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "test_data_transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "train_fer = datasets.ImageFolder(root='/tf/data/Quan/fer2013/cleaned_data/fer2013-clean/Training', transform=train_data_transform)\n",
    "val_fer = datasets.ImageFolder(root='/tf/data/Quan/fer2013/cleaned_data/fer2013-clean/PublicTest', transform=test_data_transform)\n",
    "test_fer = datasets.ImageFolder(root='/tf/data/Quan/fer2013/cleaned_data/fer2013-clean/PrivateTest/', transform=test_data_transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_fer,\n",
    "                                             batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_fer,\n",
    "                                             batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_fer,\n",
    "                                             batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels//4, 1, 1, bias = False)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(output_channels//4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(output_channels//4, output_channels//4, 3, stride, padding = 1, bias = False)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm2d(output_channels//4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(output_channels//4, output_channels, 1, 1, bias = False)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(input_channels, output_channels , 1, stride, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.bn1(x)\n",
    "        out1 = self.relu(out)\n",
    "        out = self.conv1(out1)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        if (self.input_channels != self.output_channels) or (self.stride !=1 ):\n",
    "            residual = self.conv4(out1)\n",
    "            \n",
    "        out += residual\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMasking(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim, **kwargs):\n",
    "        super(ResidualMasking, self).__init__()\n",
    "        \n",
    "        emb_dim = out_channels if emb_dim is None else emb_dim\n",
    "        \n",
    "        self.mpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # s -> s/2\n",
    "        \n",
    "        self.res1_down = ResidualBlock(in_channels, emb_dim)\n",
    "        self.res2_down = ResidualBlock(emb_dim, emb_dim)\n",
    "        \n",
    "        \n",
    "        self.res2_up = nn.Sequential(nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "                                     ResidualBlock(emb_dim, emb_dim))\n",
    "        self.res1_up = nn.Sequential(nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "                                     ResidualBlock(emb_dim, out_channels))\n",
    "        \n",
    "        self.preconv_out = nn.Sequential(nn.Conv2d(out_channels, out_channels, 1, stride=1, padding=0),\n",
    "                                         nn.BatchNorm2d(out_channels))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # s, in_channels \n",
    "        x1_down = self.res1_down(x)\n",
    "        x1_down_pool = self.mpool(x1_down)\n",
    "        \n",
    "        # s / 2, emb_dim\n",
    "        x2_down = self.res2_down(x1_down_pool)\n",
    "        x2_down_pool = self.mpool(x2_down)\n",
    "        \n",
    "        # -- s / 4 --\n",
    "        \n",
    "        # s / 2, emb_dim\n",
    "        x2_up = self.res2_up(x2_down_pool)\n",
    "        x2_up = x2_up + x1_down_pool\n",
    "        \n",
    "        # s, in_channels\n",
    "        x1_up = self.res1_up(x2_up)\n",
    "        x1_up = x1_up + x\n",
    "        \n",
    "        # out\n",
    "        x_pre_out = self.preconv_out(x1_up)\n",
    "        x_sigmoid = self.sigmoid(x_pre_out)\n",
    "        \n",
    "        return x_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=None, **kwargs):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        emb_dim = out_channels if emb_dim is None else emb_dim\n",
    "        \n",
    "        # 2 branches: trunk and residual masking\n",
    "        \n",
    "        ##   trunk branch\n",
    "        self.trunk_branch = nn.Sequential(ResidualBlock(in_channels, emb_dim),\n",
    "                                          ResidualBlock(emb_dim, out_channels))\n",
    "        \n",
    "        ##  residual masking\n",
    "        self.residual_masking = ResidualMasking(in_channels, out_channels, emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_trunk = self.trunk_branch(x)\n",
    "        x_sigmoid = self.residual_masking(x_trunk)\n",
    "        \n",
    "        x_out = (1 + x_sigmoid) * x_trunk\n",
    "        \n",
    "        x_out = x_out + x_trunk\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMaskingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualMaskingModel, self).__init__()\n",
    "        \n",
    "        self.preconv_in = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                                        nn.BatchNorm2d(64),\n",
    "                                        nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.mpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.resblock1 = ResidualBlock(64, 128)\n",
    "        self.att1 = AttentionBlock(128, 128, 64)\n",
    "        \n",
    "        self.resblock2 = ResidualBlock(128, 256)\n",
    "        self.att2 = AttentionBlock(256, 256, 128)\n",
    "        \n",
    "        self.resblock3 = ResidualBlock(256, 512)\n",
    "        self.att3 = AttentionBlock(512, 512, 256)\n",
    "        \n",
    "        self.resblock4 = ResidualBlock(512, 1024)\n",
    "        self.resblock5 = ResidualBlock(1024, 1024)\n",
    "        \n",
    "        self.preconv_out = nn.Sequential(nn.BatchNorm2d(1024),\n",
    "                                         nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Flatten(),\n",
    "                                        nn.Linear(1024, 512),\n",
    "                                        nn.BatchNorm1d(512),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(512,7),\n",
    "                                        nn.BatchNorm1d(7))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.preconv_in(x)\n",
    "        x = self.mpool(x)\n",
    "        \n",
    "        x = self.resblock1(x)\n",
    "        x = self.att1(x)\n",
    "        \n",
    "        x = self.resblock2(x)\n",
    "        x = self.att2(x)\n",
    "        \n",
    "        x = self.resblock3(x)\n",
    "        x = self.att3(x)\n",
    "        \n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)\n",
    "        \n",
    "        x = self.preconv_out(x)\n",
    "        x = self.mpool(x)\n",
    "        \n",
    "        x_out = nn.AvgPool2d(kernel_size=x.size(-1))(x)\n",
    "        x_out = self.classifier(x_out)\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(device)\n",
    "\n",
    "model = ResidualMaskingModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.004\n",
    "# reduce_factor = 2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "- Avg.loss: 1.889  | Avg.acc: 0.239\n",
      "- Avg. val_loss: 1.803  | Avg. val_acc: 0.261\n",
      "* Update optimal model\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ResidualMaskingModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ResidualBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type AttentionBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ResidualMasking. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Avg.loss: 1.733  | Avg.acc: 0.298\n",
      "- Avg. val_loss: 1.693  | Avg. val_acc: 0.352\n",
      "* Update optimal model\n",
      "Epoch:  3\n",
      "- Avg.loss: 1.561  | Avg.acc: 0.385\n",
      "- Avg. val_loss: 1.581  | Avg. val_acc: 0.367\n",
      "* Update optimal model\n",
      "Epoch:  4\n",
      "- Avg.loss: 1.464  | Avg.acc: 0.432\n",
      "- Avg. val_loss: 1.435  | Avg. val_acc: 0.444\n",
      "* Update optimal model\n",
      "Epoch:  5\n",
      "- Avg.loss: 1.401  | Avg.acc: 0.457\n",
      "- Avg. val_loss: 1.469  | Avg. val_acc: 0.439\n",
      "Epoch:  6\n",
      "- Avg.loss: 1.365  | Avg.acc: 0.474\n",
      "- Avg. val_loss: 1.507  | Avg. val_acc: 0.405\n",
      "Epoch:  7\n",
      "- Avg.loss: 1.329  | Avg.acc: 0.488\n",
      "- Avg. val_loss: 1.428  | Avg. val_acc: 0.457\n",
      "* Update optimal model\n",
      "Epoch:  8\n",
      "- Avg.loss: 1.310  | Avg.acc: 0.495\n",
      "- Avg. val_loss: 1.402  | Avg. val_acc: 0.467\n",
      "* Update optimal model\n",
      "Epoch:  9\n",
      "- Avg.loss: 1.290  | Avg.acc: 0.502\n",
      "- Avg. val_loss: 1.504  | Avg. val_acc: 0.430\n",
      "Epoch:  10\n",
      "- Avg.loss: 1.276  | Avg.acc: 0.510\n",
      "- Avg. val_loss: 1.449  | Avg. val_acc: 0.423\n",
      "Epoch:  11\n",
      "- Avg.loss: 1.265  | Avg.acc: 0.511\n",
      "- Avg. val_loss: 1.408  | Avg. val_acc: 0.475\n",
      "* Update optimal model\n",
      "Epoch:  12\n",
      "- Avg.loss: 1.253  | Avg.acc: 0.522\n",
      "- Avg. val_loss: 1.394  | Avg. val_acc: 0.460\n",
      "Epoch:  13\n",
      "- Avg.loss: 1.241  | Avg.acc: 0.528\n",
      "- Avg. val_loss: 1.293  | Avg. val_acc: 0.498\n",
      "* Update optimal model\n",
      "Epoch:  14\n",
      "- Avg.loss: 1.226  | Avg.acc: 0.531\n",
      "- Avg. val_loss: 1.215  | Avg. val_acc: 0.535\n",
      "* Update optimal model\n",
      "Epoch:  15\n",
      "- Avg.loss: 1.215  | Avg.acc: 0.537\n",
      "- Avg. val_loss: 1.197  | Avg. val_acc: 0.545\n",
      "* Update optimal model\n",
      "Epoch:  16\n",
      "- Avg.loss: 1.202  | Avg.acc: 0.542\n",
      "- Avg. val_loss: 1.372  | Avg. val_acc: 0.499\n",
      "Epoch:  17\n",
      "- Avg.loss: 1.193  | Avg.acc: 0.547\n",
      "- Avg. val_loss: 1.208  | Avg. val_acc: 0.540\n",
      "Epoch:  18\n",
      "- Avg.loss: 1.186  | Avg.acc: 0.548\n",
      "- Avg. val_loss: 1.333  | Avg. val_acc: 0.506\n",
      "Epoch:  19\n",
      "- Avg.loss: 1.186  | Avg.acc: 0.546\n",
      "- Avg. val_loss: 1.357  | Avg. val_acc: 0.526\n",
      "Epoch:  20\n",
      "- Avg.loss: 1.172  | Avg.acc: 0.558\n",
      "- Avg. val_loss: 1.273  | Avg. val_acc: 0.517\n",
      "Epoch:  21\n",
      "- Avg.loss: 1.162  | Avg.acc: 0.559\n",
      "- Avg. val_loss: 1.293  | Avg. val_acc: 0.518\n",
      "Epoch:  22\n",
      "- Avg.loss: 1.163  | Avg.acc: 0.558\n",
      "- Avg. val_loss: 1.175  | Avg. val_acc: 0.553\n",
      "* Update optimal model\n",
      "Epoch:  23\n",
      "- Avg.loss: 1.157  | Avg.acc: 0.561\n",
      "- Avg. val_loss: 1.270  | Avg. val_acc: 0.532\n",
      "Epoch:  24\n",
      "- Avg.loss: 1.148  | Avg.acc: 0.566\n",
      "- Avg. val_loss: 1.254  | Avg. val_acc: 0.525\n",
      "Epoch:  25\n",
      "- Avg.loss: 1.144  | Avg.acc: 0.566\n",
      "- Avg. val_loss: 1.275  | Avg. val_acc: 0.544\n",
      "Epoch:  26\n",
      "- Avg.loss: 1.141  | Avg.acc: 0.567\n",
      "- Avg. val_loss: 1.213  | Avg. val_acc: 0.541\n",
      "Epoch:  27\n",
      "- Avg.loss: 1.141  | Avg.acc: 0.572\n",
      "- Avg. val_loss: 1.131  | Avg. val_acc: 0.579\n",
      "* Update optimal model\n",
      "Epoch:  28\n",
      "- Avg.loss: 1.137  | Avg.acc: 0.570\n",
      "- Avg. val_loss: 1.207  | Avg. val_acc: 0.536\n",
      "Epoch:  29\n",
      "- Avg.loss: 1.132  | Avg.acc: 0.570\n",
      "- Avg. val_loss: 1.173  | Avg. val_acc: 0.562\n",
      "Epoch:  30\n",
      "- Avg.loss: 1.129  | Avg.acc: 0.576\n",
      "- Avg. val_loss: 1.155  | Avg. val_acc: 0.574\n",
      "Epoch:  31\n",
      "- Avg.loss: 1.122  | Avg.acc: 0.574\n",
      "- Avg. val_loss: 1.107  | Avg. val_acc: 0.592\n",
      "* Update optimal model\n",
      "Epoch:  32\n",
      "- Avg.loss: 1.123  | Avg.acc: 0.576\n",
      "- Avg. val_loss: 1.232  | Avg. val_acc: 0.545\n",
      "Epoch:  33\n",
      "- Avg.loss: 1.115  | Avg.acc: 0.576\n",
      "- Avg. val_loss: 1.152  | Avg. val_acc: 0.577\n",
      "Epoch:  34\n",
      "- Avg.loss: 1.113  | Avg.acc: 0.579\n",
      "- Avg. val_loss: 1.189  | Avg. val_acc: 0.550\n",
      "Epoch:  35\n",
      "- Avg.loss: 1.110  | Avg.acc: 0.581\n",
      "- Avg. val_loss: 1.197  | Avg. val_acc: 0.551\n",
      "Epoch:  36\n",
      "- Avg.loss: 1.104  | Avg.acc: 0.582\n",
      "- Avg. val_loss: 1.113  | Avg. val_acc: 0.579\n",
      "Epoch:  37\n",
      "- Avg.loss: 1.100  | Avg.acc: 0.584\n",
      "- Avg. val_loss: 1.180  | Avg. val_acc: 0.554\n",
      "Epoch:  38\n",
      "- Avg.loss: 1.098  | Avg.acc: 0.583\n",
      "- Avg. val_loss: 1.198  | Avg. val_acc: 0.546\n",
      "Epoch:  39\n",
      "- Avg.loss: 1.098  | Avg.acc: 0.586\n",
      "- Avg. val_loss: 1.096  | Avg. val_acc: 0.574\n",
      "Epoch:  40\n",
      "- Avg.loss: 1.090  | Avg.acc: 0.588\n",
      "- Avg. val_loss: 1.165  | Avg. val_acc: 0.556\n",
      "Epoch:  41\n",
      "- Avg.loss: 1.090  | Avg.acc: 0.586\n",
      "- Avg. val_loss: 1.196  | Avg. val_acc: 0.554\n",
      "Epoch:  42\n",
      "- Avg.loss: 1.091  | Avg.acc: 0.587\n",
      "- Avg. val_loss: 1.145  | Avg. val_acc: 0.561\n",
      "Epoch:  43\n",
      "- Avg.loss: 1.088  | Avg.acc: 0.588\n",
      "- Avg. val_loss: 1.131  | Avg. val_acc: 0.559\n",
      "Epoch:  44\n",
      "- Avg.loss: 1.081  | Avg.acc: 0.590\n",
      "- Avg. val_loss: 1.127  | Avg. val_acc: 0.580\n",
      "Epoch:  45\n",
      "- Avg.loss: 1.079  | Avg.acc: 0.592\n",
      "- Avg. val_loss: 1.166  | Avg. val_acc: 0.554\n",
      "Epoch:  46\n",
      "- Avg.loss: 1.081  | Avg.acc: 0.589\n",
      "- Avg. val_loss: 1.174  | Avg. val_acc: 0.547\n",
      "Epoch:  47\n",
      "- Avg.loss: 1.080  | Avg.acc: 0.592\n",
      "- Avg. val_loss: 1.096  | Avg. val_acc: 0.579\n",
      "Epoch:  48\n",
      "- Avg.loss: 1.077  | Avg.acc: 0.593\n",
      "- Avg. val_loss: 1.372  | Avg. val_acc: 0.481\n",
      "Epoch    48: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch:  49\n",
      "- Avg.loss: 1.026  | Avg.acc: 0.613\n",
      "- Avg. val_loss: 1.101  | Avg. val_acc: 0.582\n",
      "Epoch:  50\n",
      "- Avg.loss: 1.013  | Avg.acc: 0.617\n",
      "- Avg. val_loss: 1.060  | Avg. val_acc: 0.586\n",
      "Epoch:  51\n",
      "- Avg.loss: 1.006  | Avg.acc: 0.621\n",
      "- Avg. val_loss: 1.035  | Avg. val_acc: 0.613\n",
      "* Update optimal model\n",
      "Epoch:  52\n",
      "- Avg.loss: 1.002  | Avg.acc: 0.624\n",
      "- Avg. val_loss: 1.072  | Avg. val_acc: 0.599\n",
      "Epoch:  53\n",
      "- Avg.loss: 0.996  | Avg.acc: 0.622\n",
      "- Avg. val_loss: 1.074  | Avg. val_acc: 0.600\n",
      "Epoch:  54\n",
      "- Avg.loss: 0.997  | Avg.acc: 0.625\n",
      "- Avg. val_loss: 1.140  | Avg. val_acc: 0.576\n",
      "Epoch:  55\n",
      "- Avg.loss: 0.994  | Avg.acc: 0.626\n",
      "- Avg. val_loss: 1.086  | Avg. val_acc: 0.581\n",
      "Epoch:  56\n",
      "- Avg.loss: 0.987  | Avg.acc: 0.628\n",
      "- Avg. val_loss: 1.107  | Avg. val_acc: 0.591\n",
      "Epoch:  57\n",
      "- Avg.loss: 0.989  | Avg.acc: 0.628\n",
      "- Avg. val_loss: 1.036  | Avg. val_acc: 0.611\n",
      "Epoch:  58\n",
      "- Avg.loss: 0.987  | Avg.acc: 0.631\n",
      "- Avg. val_loss: 1.065  | Avg. val_acc: 0.618\n",
      "* Update optimal model\n",
      "Epoch:  59\n",
      "- Avg.loss: 0.984  | Avg.acc: 0.630\n",
      "- Avg. val_loss: 1.029  | Avg. val_acc: 0.612\n",
      "Epoch:  60\n",
      "- Avg.loss: 0.978  | Avg.acc: 0.631\n",
      "- Avg. val_loss: 1.010  | Avg. val_acc: 0.620\n",
      "* Update optimal model\n",
      "Epoch:  61\n",
      "- Avg.loss: 0.975  | Avg.acc: 0.635\n",
      "- Avg. val_loss: 1.046  | Avg. val_acc: 0.615\n",
      "Epoch:  62\n",
      "- Avg.loss: 0.969  | Avg.acc: 0.633\n",
      "- Avg. val_loss: 1.022  | Avg. val_acc: 0.613\n",
      "Epoch:  63\n",
      "- Avg.loss: 0.974  | Avg.acc: 0.636\n",
      "- Avg. val_loss: 1.066  | Avg. val_acc: 0.603\n",
      "Epoch:  64\n",
      "- Avg.loss: 0.965  | Avg.acc: 0.635\n",
      "- Avg. val_loss: 1.086  | Avg. val_acc: 0.600\n",
      "Epoch:  65\n",
      "- Avg.loss: 0.970  | Avg.acc: 0.636\n",
      "- Avg. val_loss: 1.083  | Avg. val_acc: 0.602\n",
      "Epoch:  66\n",
      "- Avg.loss: 0.963  | Avg.acc: 0.638\n",
      "- Avg. val_loss: 1.018  | Avg. val_acc: 0.626\n",
      "* Update optimal model\n",
      "Epoch:  67\n",
      "- Avg.loss: 0.954  | Avg.acc: 0.641\n",
      "- Avg. val_loss: 1.076  | Avg. val_acc: 0.606\n",
      "Epoch:  68\n",
      "- Avg.loss: 0.967  | Avg.acc: 0.640\n",
      "- Avg. val_loss: 1.046  | Avg. val_acc: 0.613\n",
      "Epoch:  69\n",
      "- Avg.loss: 0.958  | Avg.acc: 0.641\n",
      "- Avg. val_loss: 1.024  | Avg. val_acc: 0.621\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:  70\n",
      "- Avg.loss: 0.927  | Avg.acc: 0.654\n",
      "- Avg. val_loss: 0.995  | Avg. val_acc: 0.624\n",
      "Epoch:  71\n",
      "- Avg.loss: 0.909  | Avg.acc: 0.659\n",
      "- Avg. val_loss: 1.037  | Avg. val_acc: 0.616\n",
      "Epoch:  72\n",
      "- Avg.loss: 0.903  | Avg.acc: 0.660\n",
      "- Avg. val_loss: 0.992  | Avg. val_acc: 0.641\n",
      "* Update optimal model\n",
      "Epoch:  73\n",
      "- Avg.loss: 0.904  | Avg.acc: 0.663\n",
      "- Avg. val_loss: 1.054  | Avg. val_acc: 0.619\n",
      "Epoch:  74\n",
      "- Avg.loss: 0.897  | Avg.acc: 0.664\n",
      "- Avg. val_loss: 0.999  | Avg. val_acc: 0.638\n",
      "Epoch:  75\n",
      "- Avg.loss: 0.896  | Avg.acc: 0.665\n",
      "- Avg. val_loss: 1.050  | Avg. val_acc: 0.594\n",
      "Epoch:  76\n",
      "- Avg.loss: 0.893  | Avg.acc: 0.664\n",
      "- Avg. val_loss: 1.010  | Avg. val_acc: 0.619\n",
      "Epoch:  77\n",
      "- Avg.loss: 0.885  | Avg.acc: 0.669\n",
      "- Avg. val_loss: 1.071  | Avg. val_acc: 0.600\n",
      "Epoch:  78\n",
      "- Avg.loss: 0.883  | Avg.acc: 0.670\n",
      "- Avg. val_loss: 1.010  | Avg. val_acc: 0.636\n",
      "Epoch:  79\n",
      "- Avg.loss: 0.891  | Avg.acc: 0.666\n",
      "- Avg. val_loss: 0.999  | Avg. val_acc: 0.636\n",
      "Epoch:  80\n",
      "- Avg.loss: 0.878  | Avg.acc: 0.671\n",
      "- Avg. val_loss: 0.978  | Avg. val_acc: 0.633\n",
      "Epoch:  81\n",
      "- Avg.loss: 0.880  | Avg.acc: 0.670\n",
      "- Avg. val_loss: 1.013  | Avg. val_acc: 0.615\n",
      "Epoch:  82\n",
      "- Avg.loss: 0.874  | Avg.acc: 0.674\n",
      "- Avg. val_loss: 0.983  | Avg. val_acc: 0.641\n",
      "Epoch:  83\n",
      "- Avg.loss: 0.877  | Avg.acc: 0.672\n",
      "- Avg. val_loss: 1.008  | Avg. val_acc: 0.636\n",
      "Epoch:  84\n",
      "- Avg.loss: 0.874  | Avg.acc: 0.673\n",
      "- Avg. val_loss: 1.017  | Avg. val_acc: 0.620\n",
      "Epoch:  85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Avg.loss: 0.867  | Avg.acc: 0.675\n",
      "- Avg. val_loss: 1.048  | Avg. val_acc: 0.599\n",
      "Epoch:  86\n",
      "- Avg.loss: 0.870  | Avg.acc: 0.675\n",
      "- Avg. val_loss: 0.986  | Avg. val_acc: 0.644\n",
      "* Update optimal model\n",
      "Epoch:  87\n",
      "- Avg.loss: 0.865  | Avg.acc: 0.673\n",
      "- Avg. val_loss: 1.006  | Avg. val_acc: 0.609\n",
      "Epoch:  88\n",
      "- Avg.loss: 0.867  | Avg.acc: 0.678\n",
      "- Avg. val_loss: 0.982  | Avg. val_acc: 0.640\n",
      "Epoch:  89\n",
      "- Avg.loss: 0.865  | Avg.acc: 0.677\n",
      "- Avg. val_loss: 1.015  | Avg. val_acc: 0.616\n",
      "Epoch    89: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch:  90\n",
      "- Avg.loss: 0.832  | Avg.acc: 0.690\n",
      "- Avg. val_loss: 1.000  | Avg. val_acc: 0.631\n",
      "Epoch:  91\n",
      "- Avg.loss: 0.826  | Avg.acc: 0.692\n",
      "- Avg. val_loss: 1.002  | Avg. val_acc: 0.630\n",
      "Epoch:  92\n",
      "- Avg.loss: 0.824  | Avg.acc: 0.694\n",
      "- Avg. val_loss: 0.992  | Avg. val_acc: 0.646\n",
      "* Update optimal model\n",
      "Epoch:  93\n",
      "- Avg.loss: 0.820  | Avg.acc: 0.698\n",
      "- Avg. val_loss: 0.997  | Avg. val_acc: 0.629\n",
      "Epoch:  94\n",
      "- Avg.loss: 0.817  | Avg.acc: 0.696\n",
      "- Avg. val_loss: 0.974  | Avg. val_acc: 0.630\n",
      "Epoch:  95\n",
      "- Avg.loss: 0.815  | Avg.acc: 0.697\n",
      "- Avg. val_loss: 0.966  | Avg. val_acc: 0.646\n",
      "* Update optimal model\n",
      "Epoch:  96\n",
      "- Avg.loss: 0.810  | Avg.acc: 0.699\n",
      "- Avg. val_loss: 0.993  | Avg. val_acc: 0.633\n",
      "Epoch:  97\n",
      "- Avg.loss: 0.806  | Avg.acc: 0.700\n",
      "- Avg. val_loss: 0.980  | Avg. val_acc: 0.647\n",
      "* Update optimal model\n",
      "Epoch:  98\n",
      "- Avg.loss: 0.803  | Avg.acc: 0.702\n",
      "- Avg. val_loss: 1.039  | Avg. val_acc: 0.609\n",
      "Epoch:  99\n",
      "- Avg.loss: 0.803  | Avg.acc: 0.702\n",
      "- Avg. val_loss: 0.965  | Avg. val_acc: 0.645\n",
      "Epoch:  100\n",
      "- Avg.loss: 0.808  | Avg.acc: 0.702\n",
      "- Avg. val_loss: 0.979  | Avg. val_acc: 0.653\n",
      "* Update optimal model\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "model_folder = '/tf/data/Quan/fer2013/backtobasics_cleaned_data/residual_masking/'\n",
    "model_name = 'rs50_1'\n",
    "model_path = os.path.join(model_folder, model_name + '.pt')\n",
    "\n",
    "best_acc = 0.0\n",
    "curloss = 0.0\n",
    "hist = []\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    print('Epoch: ', epoch + 1)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        acc = float((torch.argmax(outputs, dim=1) == labels).float().sum()/labels.size(0))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "#         print('\\t - Step %d: loss: %.3f acc: %.3f' % (i+1, loss.item(), acc))\n",
    "\n",
    "    print('- Avg.loss: %.3f  | Avg.acc: %.3f' % (running_loss / (i+1), running_acc / (i+1)))\n",
    "    avgloss = running_loss / (i+1)\n",
    "    avgacc = running_acc / (i+1)\n",
    "    \n",
    "    # print gradient flow figure\n",
    "#     plot_grad_flow(model.named_parameters(), epoch, avgloss, avgacc,\n",
    "#                    savepath=os.path.join(model_folder, model_name + '_gf' + '_' + str(epoch) + '.png'))\n",
    "\n",
    "    # EVALUATE\n",
    "    model.eval()\n",
    "    running_valloss = 0.0\n",
    "    running_valacc = 0.0\n",
    "    for i,data in enumerate(val_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        acc = float((torch.argmax(outputs, dim=1) == labels).float().sum()/labels.size(0))\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "#         print('\\t - Step %d: loss: %.3f acc: %.3f' % (i+1, loss.item(), acc))\n",
    "\n",
    "        running_valloss += loss.item()\n",
    "        running_valacc += acc\n",
    "\n",
    "    print('- Avg. val_loss: %.3f  | Avg. val_acc: %.3f' % (running_valloss / (i+1), running_valacc / (i+1)))\n",
    "\n",
    "    avgvalloss = running_valloss / (i+1)\n",
    "    avgvalcc = running_valacc / (i+1)\n",
    "\n",
    "    hist.append([avgloss, avgvalloss, avgacc, avgvalcc])\n",
    "    \n",
    "    if best_acc < (running_valacc / (i+1)):\n",
    "        best_acc = (running_valacc / (i+1))\n",
    "        curloss = (running_valloss / (i+1))\n",
    "        torch.save(model, model_path)\n",
    "        print('* Update optimal model')\n",
    "        \n",
    "    scheduler.step(avgvalloss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  101\n",
      "- Avg.loss: 0.800  | Avg.acc: 0.705\n",
      "- Avg. val_loss: 0.986  | Avg. val_acc: 0.643\n",
      "Epoch:  102\n",
      "- Avg.loss: 0.803  | Avg.acc: 0.701\n",
      "- Avg. val_loss: 0.996  | Avg. val_acc: 0.626\n",
      "Epoch:  103\n",
      "- Avg.loss: 0.803  | Avg.acc: 0.703\n",
      "- Avg. val_loss: 0.987  | Avg. val_acc: 0.647\n",
      "Epoch:  104\n",
      "- Avg.loss: 0.799  | Avg.acc: 0.704\n",
      "- Avg. val_loss: 0.991  | Avg. val_acc: 0.627\n",
      "Epoch:  105\n",
      "- Avg.loss: 0.792  | Avg.acc: 0.707\n",
      "- Avg. val_loss: 1.069  | Avg. val_acc: 0.629\n",
      "Epoch:  106\n",
      "- Avg.loss: 0.793  | Avg.acc: 0.704\n",
      "- Avg. val_loss: 0.992  | Avg. val_acc: 0.649\n",
      "Epoch:  107\n",
      "- Avg.loss: 0.788  | Avg.acc: 0.707\n",
      "- Avg. val_loss: 0.999  | Avg. val_acc: 0.637\n",
      "Epoch:  108\n",
      "- Avg.loss: 0.796  | Avg.acc: 0.703\n",
      "- Avg. val_loss: 0.986  | Avg. val_acc: 0.650\n",
      "Epoch   108: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch:  109\n",
      "- Avg.loss: 0.780  | Avg.acc: 0.710\n",
      "- Avg. val_loss: 1.041  | Avg. val_acc: 0.633\n",
      "Epoch:  110\n",
      "- Avg.loss: 0.770  | Avg.acc: 0.715\n",
      "- Avg. val_loss: 1.047  | Avg. val_acc: 0.612\n",
      "Epoch:  111\n",
      "- Avg.loss: 0.766  | Avg.acc: 0.714\n",
      "- Avg. val_loss: 0.973  | Avg. val_acc: 0.653\n",
      "* Update optimal model\n",
      "Epoch:  112\n",
      "- Avg.loss: 0.770  | Avg.acc: 0.715\n",
      "- Avg. val_loss: 1.077  | Avg. val_acc: 0.617\n",
      "Epoch:  113\n",
      "- Avg.loss: 0.764  | Avg.acc: 0.716\n",
      "- Avg. val_loss: 1.084  | Avg. val_acc: 0.622\n",
      "Epoch:  114\n",
      "- Avg.loss: 0.766  | Avg.acc: 0.716\n",
      "- Avg. val_loss: 0.983  | Avg. val_acc: 0.648\n",
      "Epoch:  115\n",
      "- Avg.loss: 0.767  | Avg.acc: 0.717\n",
      "- Avg. val_loss: 0.973  | Avg. val_acc: 0.649\n",
      "Epoch:  116\n",
      "- Avg.loss: 0.761  | Avg.acc: 0.717\n",
      "- Avg. val_loss: 1.058  | Avg. val_acc: 0.614\n",
      "Epoch:  117\n",
      "- Avg.loss: 0.757  | Avg.acc: 0.721\n",
      "- Avg. val_loss: 1.003  | Avg. val_acc: 0.631\n",
      "Epoch   117: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch:  118\n",
      "- Avg.loss: 0.755  | Avg.acc: 0.722\n",
      "- Avg. val_loss: 0.970  | Avg. val_acc: 0.648\n",
      "Epoch:  119\n",
      "- Avg.loss: 0.752  | Avg.acc: 0.722\n",
      "- Avg. val_loss: 0.969  | Avg. val_acc: 0.652\n",
      "Epoch:  120\n",
      "- Avg.loss: 0.751  | Avg.acc: 0.725\n",
      "- Avg. val_loss: 1.061  | Avg. val_acc: 0.616\n",
      "Epoch:  121\n",
      "- Avg.loss: 0.749  | Avg.acc: 0.725\n",
      "- Avg. val_loss: 1.017  | Avg. val_acc: 0.631\n",
      "Epoch:  122\n",
      "- Avg.loss: 0.743  | Avg.acc: 0.725\n",
      "- Avg. val_loss: 0.984  | Avg. val_acc: 0.647\n",
      "Epoch:  123\n",
      "- Avg.loss: 0.743  | Avg.acc: 0.726\n",
      "- Avg. val_loss: 1.012  | Avg. val_acc: 0.634\n",
      "Epoch:  124\n",
      "- Avg.loss: 0.746  | Avg.acc: 0.723\n",
      "- Avg. val_loss: 1.022  | Avg. val_acc: 0.617\n",
      "Epoch:  125\n",
      "- Avg.loss: 0.740  | Avg.acc: 0.725\n",
      "- Avg. val_loss: 1.067  | Avg. val_acc: 0.616\n",
      "Epoch:  126\n",
      "- Avg.loss: 0.741  | Avg.acc: 0.727\n",
      "- Avg. val_loss: 0.980  | Avg. val_acc: 0.649\n",
      "Epoch   126: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch:  127\n",
      "- Avg.loss: 0.737  | Avg.acc: 0.729\n",
      "- Avg. val_loss: 0.978  | Avg. val_acc: 0.651\n",
      "Epoch:  128\n",
      "- Avg.loss: 0.736  | Avg.acc: 0.727\n",
      "- Avg. val_loss: 1.007  | Avg. val_acc: 0.630\n",
      "Epoch:  129\n",
      "- Avg.loss: 0.742  | Avg.acc: 0.727\n",
      "- Avg. val_loss: 0.995  | Avg. val_acc: 0.651\n",
      "Epoch:  130\n",
      "- Avg.loss: 0.733  | Avg.acc: 0.731\n",
      "- Avg. val_loss: 0.990  | Avg. val_acc: 0.648\n",
      "Epoch:  131\n",
      "- Avg.loss: 0.729  | Avg.acc: 0.735\n",
      "- Avg. val_loss: 0.977  | Avg. val_acc: 0.651\n",
      "Epoch:  132\n",
      "- Avg.loss: 0.734  | Avg.acc: 0.730\n",
      "- Avg. val_loss: 0.975  | Avg. val_acc: 0.654\n",
      "* Update optimal model\n",
      "Epoch:  133\n",
      "- Avg.loss: 0.731  | Avg.acc: 0.729\n",
      "- Avg. val_loss: 0.998  | Avg. val_acc: 0.636\n",
      "Epoch:  134\n",
      "- Avg.loss: 0.733  | Avg.acc: 0.728\n",
      "- Avg. val_loss: 1.047  | Avg. val_acc: 0.616\n",
      "Epoch:  135\n",
      "- Avg.loss: 0.732  | Avg.acc: 0.729\n",
      "- Avg. val_loss: 0.982  | Avg. val_acc: 0.651\n",
      "Epoch   135: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch:  136\n",
      "- Avg.loss: 0.727  | Avg.acc: 0.732\n",
      "- Avg. val_loss: 1.103  | Avg. val_acc: 0.616\n",
      "Epoch:  137\n",
      "- Avg.loss: 0.726  | Avg.acc: 0.732\n",
      "- Avg. val_loss: 1.046  | Avg. val_acc: 0.616\n",
      "Epoch:  138\n",
      "- Avg.loss: 0.727  | Avg.acc: 0.732\n",
      "- Avg. val_loss: 0.988  | Avg. val_acc: 0.635\n",
      "Epoch:  139\n",
      "- Avg.loss: 0.730  | Avg.acc: 0.733\n",
      "- Avg. val_loss: 1.029  | Avg. val_acc: 0.635\n",
      "Epoch:  140\n",
      "- Avg.loss: 0.727  | Avg.acc: 0.731\n",
      "- Avg. val_loss: 1.003  | Avg. val_acc: 0.634\n",
      "Epoch:  141\n",
      "- Avg.loss: 0.722  | Avg.acc: 0.735\n",
      "- Avg. val_loss: 1.006  | Avg. val_acc: 0.636\n",
      "Epoch:  142\n",
      "- Avg.loss: 0.726  | Avg.acc: 0.732\n",
      "- Avg. val_loss: 1.009  | Avg. val_acc: 0.632\n",
      "Epoch:  143\n",
      "- Avg.loss: 0.730  | Avg.acc: 0.731\n",
      "- Avg. val_loss: 1.006  | Avg. val_acc: 0.636\n",
      "Epoch:  144\n",
      "- Avg.loss: 0.724  | Avg.acc: 0.733\n",
      "- Avg. val_loss: 1.017  | Avg. val_acc: 0.635\n",
      "Epoch   144: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch:  145\n",
      "- Avg.loss: 0.727  | Avg.acc: 0.733\n",
      "- Avg. val_loss: 1.104  | Avg. val_acc: 0.618\n",
      "Epoch:  146\n",
      "- Avg.loss: 0.727  | Avg.acc: 0.731\n",
      "- Avg. val_loss: 0.994  | Avg. val_acc: 0.652\n",
      "Epoch:  147\n",
      "- Avg.loss: 0.729  | Avg.acc: 0.731\n",
      "- Avg. val_loss: 1.063  | Avg. val_acc: 0.633\n",
      "Epoch:  148\n",
      "- Avg.loss: 0.728  | Avg.acc: 0.730\n",
      "- Avg. val_loss: 1.006  | Avg. val_acc: 0.634\n",
      "Epoch:  149\n",
      "- Avg.loss: 0.726  | Avg.acc: 0.732\n",
      "- Avg. val_loss: 0.977  | Avg. val_acc: 0.651\n",
      "Epoch:  150\n",
      "- Avg.loss: 0.728  | Avg.acc: 0.733\n",
      "- Avg. val_loss: 0.992  | Avg. val_acc: 0.634\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "model_folder = '/tf/data/Quan/fer2013/backtobasics_cleaned_data/residual_masking/'\n",
    "model_name = 'rs50_1'\n",
    "model_path = os.path.join(model_folder, model_name + '.pt')\n",
    "\n",
    "# best_acc = 0.0\n",
    "# curloss = 0.0\n",
    "# hist = []\n",
    "\n",
    "for epoch in range(100, 150):\n",
    "\n",
    "    print('Epoch: ', epoch + 1)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        acc = float((torch.argmax(outputs, dim=1) == labels).float().sum()/labels.size(0))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc\n",
    "#         print('\\t - Step %d: loss: %.3f acc: %.3f' % (i+1, loss.item(), acc))\n",
    "\n",
    "    print('- Avg.loss: %.3f  | Avg.acc: %.3f' % (running_loss / (i+1), running_acc / (i+1)))\n",
    "    avgloss = running_loss / (i+1)\n",
    "    avgacc = running_acc / (i+1)\n",
    "    \n",
    "    # print gradient flow figure\n",
    "#     plot_grad_flow(model.named_parameters(), epoch, avgloss, avgacc,\n",
    "#                    savepath=os.path.join(model_folder, model_name + '_gf' + '_' + str(epoch) + '.png'))\n",
    "\n",
    "    # EVALUATE\n",
    "    model.eval()\n",
    "    running_valloss = 0.0\n",
    "    running_valacc = 0.0\n",
    "    for i,data in enumerate(val_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        acc = float((torch.argmax(outputs, dim=1) == labels).float().sum()/labels.size(0))\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "#         print('\\t - Step %d: loss: %.3f acc: %.3f' % (i+1, loss.item(), acc))\n",
    "\n",
    "        running_valloss += loss.item()\n",
    "        running_valacc += acc\n",
    "\n",
    "    print('- Avg. val_loss: %.3f  | Avg. val_acc: %.3f' % (running_valloss / (i+1), running_valacc / (i+1)))\n",
    "\n",
    "    avgvalloss = running_valloss / (i+1)\n",
    "    avgvalcc = running_valacc / (i+1)\n",
    "\n",
    "    hist.append([avgloss, avgvalloss, avgacc, avgvalcc])\n",
    "    \n",
    "    if best_acc < (running_valacc / (i+1)):\n",
    "        best_acc = (running_valacc / (i+1))\n",
    "        curloss = (running_valloss / (i+1))\n",
    "        torch.save(model, model_path)\n",
    "        print('* Update optimal model')\n",
    "        \n",
    "    scheduler.step(avgvalloss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
