{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the attention map between non activation vs. activation function version\n",
    "\n",
    "Observe the attention map between two version to analyze\n",
    "\n",
    "- Is the non-act attention contains negative values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(12)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non act func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpLoss(nn.Module):\n",
    "    def __init__(self, depth, residual=True):\n",
    "        super(ExpLoss, self).__init__()\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(depth, depth)\n",
    "        self.fc2 = nn.Linear(depth, depth)\n",
    "        \n",
    "#         self.main_fc = nn.Linear(depth, depth)\n",
    "        \n",
    "        self.sim_act = nn.Sigmoid()\n",
    "        self.att_act = nn.Sigmoid()\n",
    "        \n",
    "#         self.out_fc = nn.Sequential(nn.Linear(depth, depth),\n",
    "#                                     nn.BatchNorm1d(depth),\n",
    "#                                     nn.ReLU())\n",
    "\n",
    "        self.out_fc = nn.Sequential(nn.Conv2d(depth, depth, kernel_size=1, padding=0, stride=1),\n",
    "                                    nn.BatchNorm2d(depth),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        batch_size, d, h, w = x.size()\n",
    "        \n",
    "        if self.residual:\n",
    "            x_res = x\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x1 = self.fc1(x)\n",
    "        x2 = self.fc2(x)\n",
    "        \n",
    "        # cal sims (batchsize, depth)\n",
    "        sims = torch.mm(x1, x2.permute(1,0))\n",
    "        sims = sims / batch_size\n",
    "\n",
    "        mask = self.filter_mask(labels)\n",
    "        att = sims * (1 - mask.to(sims.device)) # only consider sample of same labels\n",
    "        \n",
    "        x = torch.mm(att, x_res.reshape(batch_size, d*h*w))\n",
    "        x = x.reshape(batch_size, d, h, w)\n",
    "        x = self.out_fc(x)\n",
    "    \n",
    "        if self.residual:\n",
    "            x = x + x_res\n",
    "        \n",
    "        return x, 0\n",
    "\n",
    "    def filter_mask(self, labels):\n",
    "        \"\"\"\n",
    "        zero\n",
    "        \"\"\"\n",
    "        classes = torch.unique(labels)\n",
    "        mask = torch.ones((len(labels), len(labels)))\n",
    "        \n",
    "        indices = [(labels == k).nonzero().flatten() for k in classes]\n",
    "        lindices = [torch.combinations(k, r=2, with_replacement=True) for k in indices]\n",
    "        rindices = [torch.combinations(k.flip(0), r=2, with_replacement=True) for k in indices]\n",
    "        indices = [torch.cat([lindices[i], rindices[i]]) for i in range(len(lindices))]\n",
    "\n",
    "        for k in indices:\n",
    "            mask[k[:,0], k[:,1]] = 0.\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpLoss(nn.Module):\n",
    "    def __init__(self, depth, residual=True):\n",
    "        super(ExpLoss, self).__init__()\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(depth, depth)\n",
    "        self.fc2 = nn.Linear(depth, depth)\n",
    "        \n",
    "#         self.main_fc = nn.Linear(depth, depth)\n",
    "        \n",
    "        self.sim_act = nn.Sigmoid()\n",
    "        self.att_act = nn.Sigmoid()\n",
    "        \n",
    "#         self.out_fc = nn.Sequential(nn.Linear(depth, depth),\n",
    "#                                     nn.BatchNorm1d(depth),\n",
    "#                                     nn.ReLU())\n",
    "\n",
    "        self.out_fc = nn.Sequential(nn.Conv2d(depth, depth, kernel_size=1, padding=0, stride=1),\n",
    "                                    nn.BatchNorm2d(depth),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        batch_size, d, h, w = x.size()\n",
    "        \n",
    "        if self.residual:\n",
    "            x_res = x\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x1 = self.fc1(x)\n",
    "        x2 = self.fc2(x)\n",
    "        \n",
    "        # cal sims (batchsize, depth)\n",
    "        sims = torch.mm(x1, x2.permute(1,0))\n",
    "        sims = sims / batch_size\n",
    "        sims = self.sim_act(sims)\n",
    "        mask = self.filter_mask(labels)\n",
    "        att = sims * (1 - mask.to(sims.device)) # only consider sample of same labels\n",
    "        \n",
    "        x = torch.mm(att, x_res.reshape(batch_size, d*h*w))\n",
    "        x = x.reshape(batch_size, d, h, w)\n",
    "        x = self.out_fc(x)\n",
    "    \n",
    "        if self.residual:\n",
    "            x = x + x_res\n",
    "        \n",
    "        return x, 0\n",
    "\n",
    "    def filter_mask(self, labels):\n",
    "        \"\"\"\n",
    "        zero\n",
    "        \"\"\"\n",
    "        classes = torch.unique(labels)\n",
    "        mask = torch.ones((len(labels), len(labels)))\n",
    "        \n",
    "        indices = [(labels == k).nonzero().flatten() for k in classes]\n",
    "        lindices = [torch.combinations(k, r=2, with_replacement=True) for k in indices]\n",
    "        rindices = [torch.combinations(k.flip(0), r=2, with_replacement=True) for k in indices]\n",
    "        indices = [torch.cat([lindices[i], rindices[i]]) for i in range(len(lindices))]\n",
    "\n",
    "        for k in indices:\n",
    "            mask[k[:,0], k[:,1]] = 0.\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, dff, transforms):\n",
    "        'Initialization'\n",
    "        self.transforms = transforms\n",
    "        self.dff= pd.read_csv(dff) if type(dff) is str else dff\n",
    "        \n",
    "        self.dff['pixels'] = [[int(y) for y in x.split()] for x in self.dff['pixels']]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.dff)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        #ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.dff.iloc[index]['pixels']\n",
    "#         X = X.split()\n",
    "        X = np.array(X, dtype=np.uint8)\n",
    "        X = X.reshape(48,48)\n",
    "        \n",
    "        y = int(self.dff.iloc[index]['emotion'])\n",
    "\n",
    "        if self.transforms:\n",
    "            X = self.transforms(image=X)['image']\n",
    "\n",
    "#             X = torch.cat((X,X,X),0)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "batch_size= 64\n",
    "\n",
    "df = pd.read_csv('/tf/data/Quan/fer2013/data/csv_file/fer2013.csv')\n",
    "\n",
    "df_train = df[df['Usage'] == 'Training']\n",
    "df_val = df[df['Usage'] == 'PublicTest']\n",
    "df_test = df[df['Usage'] == 'PrivateTest']\n",
    "\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "#     A.CLAHE(),\n",
    "    A.Resize(48,48),\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, p=0.5, border_mode=0, value=0),\n",
    "#     A.RandomCrop(height=40, width=40),\n",
    "    A.Normalize(mean=(0.485,), std=(0.229,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "#     A.CLAHE(),\n",
    "    A.Resize(48,48),\n",
    "    A.Normalize(mean=(0.485,), std=(0.229,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# train_set = FERDataset(df_train, train_transforms)\n",
    "train_set = FERDataset(df_train, test_transforms) # no augmentation!\n",
    "val_set = FERDataset(df_val, test_transforms)\n",
    "test_set = FERDataset(df_test, test_transforms)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                             batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                             batch_size=batch_size, shuffle=False,\n",
    "                                             num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                             batch_size=batch_size, shuffle=False,\n",
    "                                             num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sVGG_exp(nn.Module):\n",
    "    def __init__(self, features, in_features_classifier, n_classes):\n",
    "        super(sVGG_exp, self).__init__()\n",
    "        self.features_0 = features[:7]\n",
    "        self.features_1 = features[7:14]\n",
    "        self.features_2 = features[14:24]\n",
    "        self.features_3 = features[24:34]\n",
    "        \n",
    "        self.features_0[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "#         self.exploss_2 = ExpLoss(256)\n",
    "        self.exploss_3 = ExpLoss(512)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.classifier = nn.Sequential(nn.Flatten(),\n",
    "                                        nn.Linear(in_features_classifier, in_features_classifier),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(0.5, inplace=False),\n",
    "                                        nn.Linear(in_features_classifier, in_features_classifier // 2),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(0.5, inplace=False),\n",
    "                                        nn.Linear(in_features_classifier // 2, n_classes))\n",
    "        \n",
    "    def forward(self, x, labels, return_att=True):\n",
    "        x = self.features_0(x)\n",
    "        \n",
    "        x = self.features_1(x)\n",
    "        \n",
    "        x = self.features_2(x)\n",
    "#         x, _ = self.exploss_2(x, labels)\n",
    "        \n",
    "        x = self.features_3(x)\n",
    "        x, _ = self.exploss_3(x, labels)\n",
    "        \n",
    "        att = self.avgpool(x)\n",
    "        x = self.classifier(att)\n",
    "        \n",
    "        if return_att:\n",
    "            return x, 0, att\n",
    "        return x, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('exploss_more/sVGG_opt_residualexploss_lastconv_originalimgsize_noaugmentation_model.pt')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# activation = {}\n",
    "# def hook(module, input, output):\n",
    "#     activation['exploss_3'] = output\n",
    "#     return hook\n",
    "\n",
    "# model.exploss_3.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "running_valloss = 0.0\n",
    "running_valacc = 0.0\n",
    "exp_features = []\n",
    "exp_labels = []\n",
    "exp_atts = []\n",
    "for i,data in enumerate(test_loader):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "#     outputs,_, features = model(inputs, labels)\n",
    "    x = model.features_0(inputs)\n",
    "    x = model.features_1(x)\n",
    "    x = model.features_2(x)\n",
    "    x = model.features_3(x)\n",
    "    \n",
    "    x = model.exploss_3.avgpool(x)\n",
    "    x = model.exploss_3.flatten(x)\n",
    "    \n",
    "    x1 = model.exploss_3.fc1(x)\n",
    "    x2 = model.exploss_3.fc2(x)\n",
    "\n",
    "    mask = model.exploss_3.filter_mask(labels)\n",
    "\n",
    "    sims = torch.mm(x1, x2.permute(1,0))\n",
    "    sims = sims / len(test_loader)\n",
    "    att = sims * (1 - mask.to(sims.device))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         exp_features.append(features.detach().cpu())\n",
    "#         exp_labels.append(labels.detach().cpu())\n",
    "        exp_atts.append(att.detach().cpu())\n",
    "\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     acc = (torch.argmax(outputs, dim=1) == labels).float().sum()\n",
    "\n",
    "#     running_valloss += (loss.item() * inputs.size(0))\n",
    "#     running_valacc += acc.item()\n",
    "\n",
    "# print('- Avg. val_loss: %.4f | Avg. val_acc: %.4f' % (running_valloss / len(test_loader.dataset), running_valacc / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check negative value of non act\n",
    "\n",
    "> No sign of negative value, this prove that even without activation fuction such as sigmoid. The mechanism still learns effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(23.0621), tensor(2.5275), tensor(29.2517), tensor(5.4085))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_atts = torch.cat([k.flatten() for k in exp_atts])\n",
    "exp_atts.min(), exp_atts.max(), exp_atts.mean(), exp_atts.var(), exp_atts.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('exploss_more/sVGG_opt_residualexploss_sigmoid_lastconv_originalimgsize_noaugmentation_model.pt')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "running_valloss = 0.0\n",
    "running_valacc = 0.0\n",
    "exp_features = []\n",
    "exp_labels = []\n",
    "exp_atts = []\n",
    "for i,data in enumerate(test_loader):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "#     outputs,_, features = model(inputs, labels)\n",
    "    x = model.features_0(inputs)\n",
    "    x = model.features_1(x)\n",
    "    x = model.features_2(x)\n",
    "    x = model.features_3(x)\n",
    "    \n",
    "    x = model.exploss_3.avgpool(x)\n",
    "    x = model.exploss_3.flatten(x)\n",
    "    \n",
    "    x1 = model.exploss_3.fc1(x)\n",
    "    x2 = model.exploss_3.fc2(x)\n",
    "\n",
    "    mask = model.exploss_3.filter_mask(labels)\n",
    "\n",
    "    sims = torch.mm(x1, x2.permute(1,0))\n",
    "    sims = sims / len(test_loader)\n",
    "    sims = model.exploss_3.sim_act(sims)\n",
    "    att = sims * (1 - mask.to(sims.device))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         exp_features.append(features.detach().cpu())\n",
    "#         exp_labels.append(labels.detach().cpu())\n",
    "        exp_atts.append(att.detach().cpu())\n",
    "\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     acc = (torch.argmax(outputs, dim=1) == labels).float().sum()\n",
    "\n",
    "#     running_valloss += (loss.item() * inputs.size(0))\n",
    "#     running_valacc += acc.item()\n",
    "\n",
    "# print('- Avg. val_loss: %.4f | Avg. val_acc: %.4f' % (running_valloss / len(test_loader.dataset), running_valacc / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check negative value sigmoid act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.), tensor(0.1773), tensor(0.1399), tensor(0.3741))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_atts = torch.cat([k.flatten() for k in exp_atts])\n",
    "exp_atts.min(), exp_atts.max(), exp_atts.mean(), exp_atts.var(), exp_atts.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
